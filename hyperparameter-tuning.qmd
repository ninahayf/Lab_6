---
title: "hyperparameter-tuning"
author: "Nina Hayford"
email: "ninahayf@colostate.edu"
format: html
execute: 
  echo: true
---
## Load in libraries 
```{r setup, include=FALSE}
library(tidyverse)
library(tidymodels)
library(glue)
library(purrr)
library(readr)
library(powerjoin)
library(rsample)
library(recipes)
library(skimr)
library(visdat)
library(ggpubr)
library(patchwork)
library(workflowsets)
```

## Data Import/Tidy/Transform
```{r}
root <- 'https://gdex.ucar.edu/dataset/camels/file'

download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 'data/camels_attributes_v2.0.pdf')
```

```{r}
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")
remote_files  <- glue('{root}/camels_{types}.txt')
local_files   <- glue('data/camels_{types}.txt')
walk2(remote_files, local_files, download.file, quiet = TRUE)
# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE) 

camels <- power_full_join(camels ,by = 'gauge_id')

# EDA
skim(camels_combined)
vis_dat(camels_combined)
```

## Data Splitting 
```{r}
set.seed(123)
split_data <- initial_split(camels_combined, prop = 0.8)
train_data <- training(split_data) %>% filter(!is.na(q_mean))
test_data <- testing(split_data) %>% filter(!is.na(q_mean))

# Check the size of the training and testing datasets
dim(train_data)
dim(test_data)
```

## Feature Engineering 
```{r}
recipe_obj <- recipe(q_mean ~ ., data = train_data) %>%
  step_rm(gauge_lat, gauge_lon) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

recipe_prepped <- prep(recipe_obj, training = train_data)
train_data_processed <- bake(recipe_prepped, new_data = train_data)
test_data_processed <- bake(recipe_prepped, new_data = test_data)
```

## Resampling and Model Testing
### Build resamples 
```{r}
num_folds <- 10
cv_splits <- vfold_cv(train_data, v = num_folds)
```
### Build 3 Candidate Models
```{r}
rf_model <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger") %>%
  set_mode("regression")

lm_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

svm_model <- svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
  set_engine("kernlab") %>%
  set_mode("regression")

model_list <- list(rf = rf_model, lm = lm_model, svm = svm_model)
```

#### Test the Models 
```{r}
workflow_set <- workflow_set(
  preproc = list(recipe = recipe_obj), 
  models = model_list
)

results <- workflow_map(
  workflow_set, 
  resamples = cv_splits, 
  verbose = TRUE
)
```


### Model Selection
```{r}
rf_results <- extract_workflow_set_result(results, "recipe_rf")
show_best(rf_results, metric = "rsq")
```

### Model Tuning 
```{r}

```





